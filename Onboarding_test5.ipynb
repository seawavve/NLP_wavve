{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled85.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seawavve/NLP_wavve/blob/main/Onboarding_test5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tokenizer 생성"
      ],
      "metadata": {
        "id": "6Yw07_OLrjaB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9wldOSdBriDc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# 다 반드시 word_dict에 포함되게 되어있는데 왜 oov를 쓰지?\n",
        "# 대문자나 특수문자를 사전과는 다르다는걸 표기하고싶나?\n",
        "\n",
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    for sequence in sequences:\n",
        "      sequence=sequence.lower()  # 소문자로 변환\n",
        "      sequence=re.sub('[^a-zA-Z0-9 ]', '', sequence) # 특수문자 제거\n",
        "      splited_sequence = sequence.split(' ') # white space 단위 자르기\n",
        "      result.append(splited_sequence)\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    tokenized_sequences=self.preprocessing(sequences)\n",
        "    words_list=[]\n",
        "\n",
        "    # 어휘사전 생성\n",
        "    for tokenized_sequence in tokenized_sequences:\n",
        "      words_list.extend(tokenized_sequence)\n",
        "    words_set=list(set(words_list))\n",
        "    for idx in range(len(words_set)):\n",
        "      self.word_dict[words_set[idx]]=idx+1\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    transformed_sentences=[]\n",
        "\n",
        "    if self.fit_checker:\n",
        "      for token in tokens:\n",
        "        transformed_sentence=[]\n",
        "        for idx in range(len(token)):\n",
        "          if token[idx] in self.word_dict:\n",
        "            transformed_sentence.append(self.word_dict[token[idx]])\n",
        "          else:\n",
        "            transformed_sentence.append(self.word_dict['oov'])\n",
        "        transformed_sentences.append(transformed_sentence)\n",
        "      return transformed_sentences\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    print(result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "sentences = ['I go to school.', 'I LIKE pizza!']\n",
        "test_sentences = ['I like to PARTY','miss you']\n",
        "# print(tokenizer.preprocessing(sentences))\n",
        "# print('tokenizer.fit(sentences)')\n",
        "# print(tokenizer.fit(sentences))\n",
        "# print('tokenizer.transform(sentences)')\n",
        "# print(tokenizer.transform(sentences))\n",
        "\n",
        "print(tokenizer.fit_transform(test_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDOiI5Dkup-s",
        "outputId": "64aa12f7-f8d2-4e11-ca2b-79b0582bcf23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 6, 3, 2], [5, 4]]\n",
            "[[1, 6, 3, 2], [5, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.TfidfVectorizer 생성하기"
      ],
      "metadata": {
        "id": "oW3XJvsAzPeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고자료\n",
        "https://wikidocs.net/31698"
      ],
      "metadata": {
        "id": "AXmjQNmLBmmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    result=list()\n",
        "    print(tokenized)\n",
        "    #IDF 행렬 만들기\n",
        "    # 단어사전을 기준으로 하나씩 리스트로 넣는걸까\n",
        "    #왜 0이 나오지?\n",
        "    for word in self.tokenizer.word_dict:\n",
        "      df = 0\n",
        "      print(word)\n",
        "      for sequence in sequences:\n",
        "        print(sequence)\n",
        "        if word in sequence:\n",
        "          df += 1\n",
        "      result.append(df)\n",
        "    # print(result)\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      #TF-iDF 행렬 만들기\n",
        "      '''\n",
        "      문제 2-2.\n",
        "      '''\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "hQNrp2fzzWQK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_tokenizer = TfidfVectorizer(tokenizer)\n",
        "sentences = ['I go to school.', 'I LIKE pizza!']\n",
        "test_sentences = ['I like to PARTY','miss you']\n",
        "print(tfidf_tokenizer.fit(test_sentences))"
      ],
      "metadata": {
        "id": "4r61MW25zWiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486e164b-dbb2-4c69-a59d-5be7ef5f7785"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 6, 3, 2], [5, 4]]\n",
            "[[1, 6, 3, 2], [5, 4]]\n",
            "oov\n",
            "I like to PARTY\n",
            "miss you\n",
            "i\n",
            "I like to PARTY\n",
            "miss you\n",
            "party\n",
            "I like to PARTY\n",
            "miss you\n",
            "to\n",
            "I like to PARTY\n",
            "miss you\n",
            "you\n",
            "I like to PARTY\n",
            "miss you\n",
            "miss\n",
            "I like to PARTY\n",
            "miss you\n",
            "like\n",
            "I like to PARTY\n",
            "miss you\n",
            "[0, 2, 0, 1, 1, 1, 1]\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PDUAGH7ozWTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}