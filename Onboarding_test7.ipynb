{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled85.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seawavve/NLP_wavve/blob/main/Onboarding_test7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tokenizer 생성"
      ],
      "metadata": {
        "id": "6Yw07_OLrjaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "제출전 확인할 사항\n",
        "* 클린코드 유의\n",
        "* = 사이 띄어쓰기로 가독성 높이기\n",
        "* word_dict, dict_word 이런 변수명 순서 통일성"
      ],
      "metadata": {
        "id": "yVSpuB2KxWR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "9wldOSdBriDc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# fit이 일어난 후에 transform이 일어난다면 반드시 모든 단어가 word_dict에 포함되게 되어있는데 왜 oov를 쓰지?\n",
        "\n",
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  # 텍스트 전처리\n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    for sequence in sequences:\n",
        "      sequence=sequence.lower()  # 소문자로 변환\n",
        "      sequence=re.sub('[^a-zA-Z0-9 ]', '', sequence) # 특수문자 제거\n",
        "      splited_sequence = sequence.split(' ') # white space 단위 자르기\n",
        "      result.append(splited_sequence)\n",
        "    return result\n",
        "  \n",
        "  # 어휘 사전을 구축\n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    tokenized_sequences=self.preprocessing(sequences)\n",
        "    words_list=[]\n",
        "\n",
        "    # 어휘사전 생성\n",
        "    for tokenized_sequence in tokenized_sequences:\n",
        "      words_list.extend(tokenized_sequence)\n",
        "    words_set=sorted(list(set(words_list)))\n",
        "    for idx in range(len(words_set)):\n",
        "      word = words_set[idx]\n",
        "      self.word_dict[word]=idx+1\n",
        "    self.fit_checker = True\n",
        "    print(self.word_dict)\n",
        "  \n",
        "  # 어휘 사전을 활용하여 입력 문장을 정수 인덱싱\n",
        "  def transform(self, sequences):\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    transformed_sentences=[]\n",
        "\n",
        "    if self.fit_checker:\n",
        "      for token in tokens:\n",
        "        transformed_sentence=[]\n",
        "        for idx in range(len(token)):\n",
        "          word = token[idx]\n",
        "          if word in self.word_dict:\n",
        "            transformed_sentence.append(self.word_dict[word])\n",
        "          else:\n",
        "            transformed_sentence.append(self.word_dict['oov'])\n",
        "        transformed_sentences.append(transformed_sentence)\n",
        "      return transformed_sentences\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "sentences = ['I go to school.', 'I LIKE pizza!']\n",
        "test_sentences = ['I like to PARTY','miss you']\n",
        "\n",
        "print(tokenizer.fit_transform(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDOiI5Dkup-s",
        "outputId": "4b314963-b7fd-449e-cead-3dc2b8856589"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'oov': 0, 'go': 1, 'i': 2, 'like': 3, 'pizza': 4, 'school': 5, 'to': 6}\n",
            "[[2, 1, 6, 5], [2, 3, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.TfidfVectorizer 생성하기"
      ],
      "metadata": {
        "id": "oW3XJvsAzPeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고자료\n",
        "https://wikidocs.net/31698"
      ],
      "metadata": {
        "id": "AXmjQNmLBmmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "\n",
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "    self.tfidf_matrix = list()\n",
        "    self.idf_matrix = list()\n",
        "  \n",
        "  #대문자는 df 처리가 안된다. 노린문제인가? 이런경우 어떻게 처리하는가?\n",
        "  #idf부터 모두 토큰화된걸 기준으로 계산하면 된다.\n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    N=len(sequences)\n",
        "\n",
        "    #IDF 행렬 만들기 (idx기준으로 바꾸기)---------------------------\n",
        "    for idx in range(len(self.tokenizer.word_dict)):\n",
        "      print(idx)\n",
        "      df = 0\n",
        "      for tokenized_sequence in tokenized:\n",
        "        if idx in tokenized_sequence:\n",
        "          df += 1\n",
        "      print(idx, df)\n",
        "      self.idf_matrix.append( log(N/(df+1)) )\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      print(tokenized)\n",
        "\n",
        "      #TF-iDF 행렬 만들기\n",
        "      for tokenized_sequence in tokenized :\n",
        "        tf_list=[]\n",
        "        for idx in range(len(self.idf_matrix)):\n",
        "          tf = tokenized_sequence.count(idx)\n",
        "          idf = self.idf_matrix[idx]\n",
        "          tf_idf = tf*idf\n",
        "          tf_list.append( tf_idf )\n",
        "        self.tfidf_matrix.append(tf_list)\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "hQNrp2fzzWQK"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_tokenizer = TfidfVectorizer(tokenizer)\n",
        "sentences = ['I go to school.', 'I LIKE pizza!']\n",
        "test_sentences = ['I like to PARTY','miss you', 'I Love you baby!']\n",
        "print(tfidf_tokenizer.fit(test_sentences))\n",
        "print(tfidf_tokenizer.transform(test_sentences))"
      ],
      "metadata": {
        "id": "4r61MW25zWiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faca9493-7498-4ef5-f584-f58e5edafe99"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'oov': 0, 'go': 1, 'i': 2, 'like': 3, 'pizza': 4, 'school': 5, 'to': 7, 'baby': 1, 'love': 4, 'miss': 5, 'party': 6, 'you': 8}\n",
            "0\n",
            "0 0\n",
            "1\n",
            "1 1\n",
            "2\n",
            "2 2\n",
            "3\n",
            "3 1\n",
            "4\n",
            "4 1\n",
            "5\n",
            "5 1\n",
            "6\n",
            "6 1\n",
            "7\n",
            "7 1\n",
            "8\n",
            "8 2\n",
            "9\n",
            "9 0\n",
            "10\n",
            "10 0\n",
            "11\n",
            "11 0\n",
            "None\n",
            "[[2, 3, 7, 6], [5, 8], [2, 4, 8, 1]]\n",
            "[[0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.4054651081081644, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PDUAGH7ozWTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}